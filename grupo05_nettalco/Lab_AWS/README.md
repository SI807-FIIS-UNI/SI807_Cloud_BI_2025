# ğŸ§ª Laboratorio GRPO 05 AWS â€“ ConfiguraciÃ³n Inicial 
En este laboratorio enseÃ±an a implementar un pipeline de datos usando S3, Glue, IAM y Athena sobre AWS. 
Paso a paso, se desarrollara desde la configuraciÃ³n segura del almacenamiento, hasta la automatizaciÃ³n del catÃ¡logo,
la transformaciÃ³n eficiente y el anÃ¡lisis con consultas SQL.

## ğŸ—‚ï¸ 1. CreaciÃ³n del Bucket S3
El primer paso consistiÃ³ en crear un bucket S3 que servirÃ¡ como almacenamiento principal para los datos utilizados en el laboratorio.  
Este bucket serÃ¡ el origen desde el cual AWS Glue obtendrÃ¡ los archivos para el proceso de catalogaciÃ³n y anÃ¡lisis.
El bucket de S3 funciona como data lake. AhÃ­ almacenan tanto los datos crudos (raw) como los procesados (curated).

### ğŸ”§Completamos los campos solicitados por AWS
- **Bucket name:** `s3-grupo-5-vf`  
- **AWS Region:** `sa-east-1` (SudamÃ©rica â€“ SÃ£o Paulo)  
- **Block Public Access:** Habilitado  
- **Bucket versioning:** Deshabilitado  
- **Default encryption:** Deshabilitado  

Dentro del bucket, se creÃ³ la siguiente estructura de carpetas:

```
â”œâ”€â”€ data/
â”‚Â Â  â””â”€â”€ raw/
â”œâ”€â”€ evidencias/
â”œâ”€â”€ script/
â””â”€â”€ README.md
```
![Bucket](/grupo05_nettalco/Lab_AWS/evidencias/S3_archive_subidos.jpg)

# ğŸ¤– 2. ConfiguraciÃ³n del Crawler en AWS Glue
A continuaciÃ³n, se utiliza un crawler de Glue para explorar automÃ¡ticamente la estructura de los datos y alimentar el Glue Data Catalog con los metadatos.
## âš™ï¸ Campos configurados al crear el Crawler

Se completaron los siguientes campos requeridos:
- **Name**: crawler_grupo5

- **Data source**: S3

- **S3 path**: s3://s3-grupo-5-vf/archive/

- **IAM role**: AWSGlueServiceRole-grupo5 (rol creado con permisos especÃ­ficos para acceder al bucket)

- **Schedule**: Ejecutar bajo demanda (no programado automÃ¡ticamente)

- **Database**: base_prueba (base de datos creada en Glue para almacenar los metadatos)

- **Output**: Sobrescribir tablas existentes en caso de cambios detectados


Una vez se completa la configuraciÃ³n del crawler, lo ejecutan manualmente para que explore el bucket, detecte el archivo 'Amazon Sale Report.csv' y genere automÃ¡ticamente en el Glue Data Catalog una tabla con la estructura de columnas y tipos de datos correspondiente. De esta manera, establecen un 
esquema organizado que facilita futuras etapas de procesamiento y anÃ¡lisis.

![Crawler](/grupo05_nettalco/Lab_AWS/evidencias/Creacion_crawler.jpg)
