# üß™ Laboratorio GRPO 05 AWS ‚Äì Configuraci√≥n Inicial 
En este laboratorio ense√±an a implementar un pipeline de datos usando S3, Glue, IAM y Athena sobre AWS. 
Paso a paso, se desarrollara desde la configuraci√≥n segura del almacenamiento, hasta la automatizaci√≥n del cat√°logo,
la transformaci√≥n eficiente y el an√°lisis con consultas SQL.

## üóÇÔ∏è 1. Creaci√≥n del Bucket S3
El primer paso consisti√≥ en crear un bucket S3 que servir√° como almacenamiento principal para los datos utilizados en el laboratorio.  
Este bucket ser√° el origen desde el cual AWS Glue obtendr√° los archivos para el proceso de catalogaci√≥n y an√°lisis.
El bucket de S3 funciona como data lake. Ah√≠ almacenan tanto los datos crudos (raw) como los procesados (curated).

### üîßCompletamos los campos solicitados por AWS
- **Bucket name:** `s3-grupo-5-vf`  
- **AWS Region:** `sa-east-1` (Sudam√©rica ‚Äì S√£o Paulo)  
- **Block Public Access:** Habilitado  
- **Bucket versioning:** Deshabilitado  
- **Default encryption:** Deshabilitado  

Dentro del bucket, se cre√≥ la siguiente estructura de carpetas:

```
‚îú‚îÄ‚îÄ data/
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw/
‚îú‚îÄ‚îÄ evidencias/
‚îú‚îÄ‚îÄ script/
‚îî‚îÄ‚îÄ README.md
```
![Bucket](/grupo05_nettalco/Lab_AWS/evidencias/S3_archive_subidos.jpg)

# ü§ñ 2. Configuraci√≥n del Crawler en AWS Glue
A continuaci√≥n, se utiliza un crawler de Glue para explorar autom√°ticamente la estructura de los datos y alimentar el Glue Data Catalog con los metadatos.
## ‚öôÔ∏è Campos configurados al crear el Crawler

Se completaron los siguientes campos requeridos:
- **Name**: crawler_grupo5

- **Data source**: S3

- **S3 path**: s3://s3-grupo-5-vf/archive/

- **IAM role**: AWSGlueServiceRole-grupo5 (rol creado con permisos espec√≠ficos para acceder al bucket)

- **Schedule**: Ejecutar bajo demanda (no programado autom√°ticamente)

- **Database**: base_prueba (base de datos creada en Glue para almacenar los metadatos)

- **Output**: Sobrescribir tablas existentes en caso de cambios detectados


Una vez se completa la configuraci√≥n del crawler, lo ejecutan manualmente para que explore el bucket, detecte el archivo 'Amazon Sale Report.csv' y genere autom√°ticamente en el Glue Data Catalog una tabla con la estructura de columnas y tipos de datos correspondiente. De esta manera, establecen un 
esquema organizado que facilita futuras etapas de procesamiento y an√°lisis.

![Crawler](/grupo05_nettalco/Lab_AWS/evidencias/Creacion_crawler.jpg)

# üîê 3. Configuraci√≥n de IAM Policy

Ahora nos toca definir una pol√≠tica IAM que le d√© al rol de Glue los permisos necesarios para trabajar con el bucket S3. En este punto debemos asegurarnos de que el rol pueda listar, leer, escribir y eliminar objetos, pero sin otorgar permisos que no sean necesarios.

Una vez que tenemos claro qu√© necesita acceder el Crawler, procedemos a crear la pol√≠tica con los permisos m√≠nimos. Despu√©s de este paso, asignamos esta pol√≠tica al rol que utilizar√° Glue, de modo que el servicio pueda interactuar correctamente con el bucket durante la ejecuci√≥n de los crawlers y jobs.


## üìú Pol√≠tica IAM utilizada
```json
{
    "Version": "2012-10-17",
    "statement": [
        {
            "Effect": "Allow",
            "Action": [
              "s3:GetObject",
              "s3:Putobject",
              "s3:ListBucket",
              "s3:Deleteobject"
            ],
            "Resource": [
                "arn:aws:s3:::s3-grupo-5-vf/*"
            ],
            "Condition": {
                "StringEquals": {
                    "aws: ResourceAccount": "581983650106"
                }
            }
        }
    ]
}
```
Otorgan permisos de lectura, escritura, listado y eliminaci√≥n solo sobre el bucket y sus subcarpetas. As√≠, aplican el principio de privilegios m√≠nimos.

![IAM](/grupo05_nettalco/Lab_AWS/evidencias/Politicas_IAM.jpg)


# üîé Aspectos destacados

"Action" Define las operaciones permitidas sobre los recursos de S3:

- **GetObject**: Leer objetos del bucket

- **PutObject**: Escribir nuevos objetos

- **ListBucket**: Listar el contenido del bucket

- **DeleteObject**: Eliminar objetos existentes

- **Resource**: Especifica los recursos de S3 a los cuales el rol tiene acceso.
Incluye el bucket principal (s3-grupo-5-vf) y sus subrutas dentro de archive/.


# ‚öôÔ∏è 4. Desarrollo del Script de Transformaci√≥n (AWS Glue Job)

En esta parte avanzamos con la creaci√≥n de un **script en Python** que ser√° ejecutado por un **Glue Job**. Primero configuramos el job para que pueda leer los datos almacenados en el bucket S3; luego incorporamos las transformaciones necesarias dentro del script y, finalmente, preparamos la salida en formato **Parquet**, que es el formato ideal para consultarlo desde **Athena**.

Despu√©s de completar esta etapa, lo que sigue es validar que el job se ejecute sin errores y que los archivos Parquet generados est√©n correctamente organizados en el bucket.

![Evidencia job](/grupo06_scotiabank/Lab_AWS/evidences/Evidencia_04_Job.png)

En esta parte nos enfocamos en definir claramente qu√© har√° el script del Glue Job. Aqu√≠ debemos indicar que el objetivo del c√≥digo es limpiar los datos, renombrar columnas, normalizar formatos de fecha, convertir tipos de datos y generar particiones por a√±o y mes, con el fin de mejorar el rendimiento en las consultas posteriores.

Una vez descrito esto, lo siguiente es configurar los par√°metros que el job necesita para funcionar correctamente. En este punto debemos establecer, por ejemplo:

- **--SOURCE**: Archivo a consumir

- **--TARGET**: Archivo de salida

![Par√°metros](/grupo05_nettalco/Lab_AWS/evidencias/Script_parametros.jpg)

## üß© Script Python ‚Äì Transformaci√≥n de datos
```python
import sys
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from pyspark.context import SparkContext
from pyspark.sql import functions as F

args = getResolvedOptions(sys.argv, ["JOB_NAME","SOURCE","TARGET"])
sc = SparkContext()
glue = GlueContext(sc)
spark = glue.spark_session

print("üöÄ Iniciando Glue Job de transformaci√≥n...")

df = (spark.read
      .option("header", True)
      .option("inferSchema", True)
      .csv(args["SOURCE"]))

print(f"üìä Registros iniciales: {df.count()}")

df2 = (df
    .withColumnRenamed("index", "index_id")
    .withColumnRenamed("Order ID", "order_id")
    .withColumnRenamed("Date", "order_date_raw")
    .withColumnRenamed("Status", "order_status")
    .withColumnRenamed("Fulfilment", "fulfilment_type")
    .withColumnRenamed("Sales Channel", "sales_channel")
    .withColumnRenamed("ship-service-level", "ship_service_level")
    .withColumnRenamed("Style", "style")
    .withColumnRenamed("SKU", "sku")
    .withColumnRenamed("Category", "category")
    .withColumnRenamed("Size", "size")
    .withColumnRenamed("ASIN", "asin")
    .withColumnRenamed("Courier Status", "courier_status")
    .withColumnRenamed("Qty", "quantity")
    .withColumnRenamed("currency", "currency_code")
    .withColumnRenamed("Amount", "amount")
    .withColumnRenamed("ship-city", "ship_city")
    .withColumnRenamed("ship-state", "ship_state")
    .withColumnRenamed("ship-postal-code", "ship_postal_code")
    .withColumnRenamed("ship-country", "ship_country")
    .withColumnRenamed("promotion-ids", "promotion_ids")
    .withColumnRenamed("B2B", "is_b2b")
    .withColumnRenamed("fulfilled-by", "fulfilled_by")
    # üß© Convertir fechas con m√∫ltiples formatos posibles
    .withColumn(
        "order_date",
        F.coalesce(
            F.to_date("order_date_raw", "MM-dd-yy"),
            F.to_date("order_date_raw", "M/d/yyyy"),
            F.to_date("order_date_raw", "MM/dd/yyyy")
        )
    )

# üîç Crear particiones derivadas
    .withColumn("anio", F.year("order_date"))
    .withColumn("mes", F.date_format("order_date", "MM"))
    # üí∞ Convertir monto num√©rico
    .withColumn("amount_numeric", F.col("amount").cast("double"))
    # ‚öôÔ∏è Filtrar registros con fechas v√°lidas
    .filter(F.col("order_date").isNotNull())
    # Eliminar celdas vac√≠as en la columna de montos
    .withColumn(
        "amount",
        F.when(F.col("amount").cast("double").isNotNull(), F.col("amount").cast("double"))
         .otherwise(F.lit(0.0))
    )
)

(df2.write.mode("overwrite")
    .partitionBy("anio","mes")
    .parquet(args["TARGET"]))

print("‚úÖ Transformaci√≥n completada. Datos guardados en ruta de destino.")

```
![Evidencia Ejecucion](/grupo05_nettalco/Lab_AWS/evidencias/Script_ejecuccion_exitosa.jpg)



