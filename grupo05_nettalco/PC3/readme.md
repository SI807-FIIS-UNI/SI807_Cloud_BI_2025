# üß™ Evidencias de despliegue
A continuaci√≥n, se presentan las evidencias de la implementaci√≥n y el procesamiento de datos realizado en Google Cloud Platform (GCP).

**LINK VIDEO EXPLICATIVO:**  [üëâ VIDEO-PC3](./evidencias_pc3/Video-PC3.mp4)


Esta secci√≥n ilustra el resultado del trabajo, que involucr√≥ el procesamiento de los datasets de la empresa Nettalco para el proyecto del Parcial, utilizando la infraestructura de Big Data desplegada en GCP.

## üóÇÔ∏è 1. Google Cloud Storage 
## üíæ Configuraci√≥n de Google Cloud Storage (GCS)

GCS actuar√° como nuestro sistema de archivos distribuido (donde almacenaremos los datos y los *scripts*).

### a. **Crear un *Bucket* de GCS**

Un *bucket* es el contenedor fundamental de almacenamiento de objetos en GCS.

1.  En la barra de b√∫squeda, escribimos y seleccionamos **"Cloud Storage"**.
2.  Hacemos clic en **"Crear bucket"**.
3.  Se asigna un nombre **√∫nico y global**
4.  Se selecciona la **Regi√≥n** donde se desplegar√° el cl√∫ster de Dataproc.
5.  Configuramos las opciones de privacidad y protecci√≥n .
6.  Hacemos clic en **"Crear"**.

![captura bucket](/grupo05_nettalco/PC3/evidencias_pc3/img_001.png)

### b. **Subir Archivos de Prueba (*Scripts* y *Datasets*)**

Una vez creado el *bucket*, subiremos los archivos que el cl√∫ster usar√° para el procesamiento.

1.  Dentro de tu *bucket*, haz clic en **"Subir archivos"** o arrastra los archivos al navegador de carpetas.
2.  Selecciona los *scripts* de Spark y los *datasets* de prueba desde tu m√°quina local.
3.  Haz clic en **"Abrir"** para iniciar la subida.

**Evidencia:** El *bucket* `nettalco-data-bd_grupo05` con los archivos CSV y los directorios de trabajo listos para ser utilizados por Dataproc.

![bucket GCS con CSV](/grupo05_nettalco/PC3/evidencias_pc3/img_002.png)

---

## üóÇÔ∏è 2. Cloud Shell
En este paso, utilizaremos la herramienta de l√≠nea de comandos `gcloud` a trav√©s de **Cloud Shell** para construir y lanzar el cl√∫ster.

### a. **Inicializar Cloud Shell y Desplegar el Cl√∫ster**

1.  Navega al servicio **Dataproc** en la Consola de GCP.
2.  Haz clic en el icono **Cloud Shell** (terminal en la web) en la esquina superior derecha de la Consola.
3.  Una vez en Cloud Shell, ejecuta el comando de creaci√≥n del cl√∫ster (mostrado a continuaci√≥n) para iniciar el despliegue de los recursos.

**Evidencia:** El Cloud Shell activo, mostrando los comandos de `gcloud dataproc clusters create` utilizados para configurar el entorno.

![3](/grupo05_nettalco/PC3/evidencias_pc3/img_003.png)

### b. **Comando de Despliegue**

Utilizaremos la herramienta de l√≠nea de comandos gcloud en Cloud Shell o en la  terminal local para desplegar el cl√∫ster.

```bash
gcloud dataproc clusters create nettalco-cluster \
    --region=us-east1 \
    --zone=us-east1-c \
    --master-machine-type=n1-standard-2 \
    --master-boot-disk-size=100 \
    --num-workers=2 \
    --worker-machine-type=n1-standard-2 \
    --worker-boot-disk-size=100 \
    --image-version=2.1-debian11 \
    --bucket=nettalco-data-bd_grupo05 \
    --optional-components=JUPYTER \
    --enable-component-gateway \
    --max-idle=336h \
    --project=nettalco-data-478503
```

![4](/grupo05_nettalco/PC3/evidencias_pc3/img_004.png)

## üóÇÔ∏è 3. Ejecuci√≥n de Trabajos y Procesamiento de Datos(Dataproc) 

Una vez que el cl√∫ster `nettalco-cluster` est√° activo y los datos se encuentran en GCS, procedemos a ejecutar el *script* de Spark que realiza las transformaciones y la carga final de los datos. Utilizamos **JupyterLab** para la ejecuci√≥n interactiva.

### a. **Ejecuci√≥n del *Script* de Spark en JupyterLab**

El primer paso es ejecutar el *notebook* que contiene el c√≥digo de **PySpark**. Este c√≥digo lee los archivos CSV de Nettalco desde GCS, realiza las transformaciones y prepara los *datasets* para el an√°lisis.

**Evidencia:** El entorno JupyterLab activo, mostrando el *notebook* `Procesamiento_nettalco.ipynb` con el c√≥digo PySpark listo para la lectura y transformaci√≥n de datos.

![Notebook_JupyterLab](/grupo05_nettalco/PC3/evidencias_pc3/img_006.png)

### b. **Carga de Datos Procesados a BigQuery**

Una vez transformados los datos con Spark, el *job* se encarga de cargarlos en BigQuery para su posterior consumo por herramientas de BI como Looker Studio. El cl√∫ster utiliza conectores Spark-BigQuery para realizar esta operaci√≥n masiva.

**Evidencia:** La terminal de JupyterLab mostrando los comandos de `bq load` o los resultados de las operaciones de carga de Spark, confirmando el estado **DONE** (Completado) para m√∫ltiples tablas de Nettalco.

![Terminal de JupyterLab](/grupo05_nettalco/PC3/evidencias_pc3/img_007.png)

> **Nota:** La evidencia muestra la exitosa finalizaci√≥n de la carga de *datasets* clave como `ventas_volumen_ventas_por_cliente`, `eficiencia_operativa`, e `indice_ventas_cliente`.

---

# üóÇÔ∏è 4. BigQuery

Despu√©s del procesamiento en Dataproc, los resultados fueron cargados en
BigQuery dentro del dataset `ventas_nettalco`.\
Esta secci√≥n detalla las tablas finales creadas, su estructura y las
consultas SQL utilizadas para validar la consistencia de los datos
transformados.

------------------------------------------------------------------------

## üìå 4.1 Tablas creadas en BigQuery

Tras ejecutar los comandos `bq load`, el dataset `ventas_nettalco` qued√≥
conformado por **9 tablas finales**, cada una derivada de procesos
PySpark en Dataproc:

| Tabla                                   | Descripci√≥n Detallada                                                    |
|-----------------------------------------|--------------------------------------------------------------------------|
| **total_prendas_por_talla**             | Cantidad total de prendas producidas agrupadas seg√∫n cada talla          |
| **volumen_ventas_por_cliente**          | Volumen acumulado de prendas entregadas por cada cliente                 |
| **fecha_ventas**                        | Registro diario de ventas procesadas por fecha                           |
| **tendencias_ventas_por_franja_horaria**| An√°lisis de ventas por franjas horarias (ma√±ana, tarde, noche)           |
| **productos_mas_vendidos**              | Identificaci√≥n y ranking de los estilos con mayor volumen de ventas       |
| **eficiencia_operativa**                | Proporci√≥n de eficiencia basada en fallas vs inspecciones realizadas      |
| **indice_ventas_cliente**               | Ventas por cliente, desglosadas por l√≠nea de producto                     |
| **prediccion_ventas**                   | Tendencias hist√≥ricas con c√°lculo del promedio m√≥vil de 7 periodos        |
| **comportamiento_clientes**             | M√©tricas de comportamiento: frecuencia de compra y promedio de prendas    |


![10](/grupo05_nettalco/PC3/evidencias_pc3/img_010.png)

------------------------------------------------------------------------

## üì• 4.2 Evidencia de la carga en BigQuery

A continuaci√≥n se detallan los comandos utilizados para cargar cada una
de las tablas procesadas desde Google Cloud Storage hacia el dataset
`ventas_nettalco` en BigQuery.\
Cada comando utiliza `--autodetect` para permitir que BigQuery
identifique de manera autom√°tica los tipos de datos de cada columna.

------------------------------------------------------------------------

### 1. **Total prendas por talla**

``` bash
bq load --source_format=CSV --autodetect ventas_nettalco.total_prendas_por_talla \
gs://nettalco-data-bd_grupo05/curated/total_prendas_por_talla/*.csv
```

### 2. **Volumen de ventas por cliente**

``` bash
bq load --source_format=CSV --autodetect ventas_nettalco.volumen_ventas_por_cliente \
gs://nettalco-data-bd_grupo05/curated/volumen_ventas_por_cliente/*.csv
```

### 3. **Fecha ventas**

``` bash
bq load --source_format=CSV --autodetect ventas_nettalco.fecha_ventas \
gs://nettalco-data-bd_grupo05/curated/fecha_ventas/*.csv
```

### 4. **Tendencias por franja horaria**

``` bash
bq load --source_format=CSV --autodetect ventas_nettalco.tendencias_ventas_por_franja_horaria \
gs://nettalco-data-bd_grupo05/curated/tendencias_ventas_por_franja_horaria/*.csv
```

### 5. **Productos m√°s vendidos**

``` bash
bq load --source_format=CSV --autodetect ventas_nettalco.productos_mas_vendidos \
gs://nettalco-data-bd_grupo05/curated/productos_mas_vendidos/*.csv
```

### 6. **Eficiencia operativa**

``` bash
bq load --source_format=CSV --autodetect ventas_nettalco.eficiencia_operativa \
gs://nettalco-data-bd_grupo05/curated/eficiencia_operativa/*.csv
```

### 7. **√çndice de ventas por cliente y l√≠nea**

``` bash
bq load --source_format=CSV --autodetect ventas_nettalco.indice_ventas_cliente \
gs://nettalco-data-bd_grupo05/curated/indice_ventas_cliente/*.csv
```

### 8. **Predicci√≥n de ventas**

``` bash
bq load --source_format=CSV --autodetect ventas_nettalco.prediccion_ventas \
gs://nettalco-data-bd_grupo05/curated/prediccion_ventas/*.csv
```

### 9. **Comportamiento de clientes**

``` bash
bq load --source_format=CSV --autodetect ventas_nettalco.comportamiento_clientes \
gs://nettalco-data-bd_grupo05/curated/comportamiento_clientes/*.csv
```

------------------------------------------------------------------------

La carga fue realizada desde el nodo maestro v√≠a `bq load`, confirmando
el estado **DONE** en todas las tablas:

    Current status: DONE

------------------------------------------------------------------------

## üîé 4.3 Validaci√≥n de datos en BigQuery

Se realizaron consultas para verificar:

-   Estructura
-   Tipos detectados
-   Calidad de datos
-   Integridad de agregaciones
-   Outliers

------------------------------------------------------------------------

# üß™ 4.4 Consultas SQL de validaci√≥n

## ‚úÖ A) Validar conteo de registros por tabla

``` sql
SELECT
  table_id,
  row_count,
  size_bytes,
  TIMESTAMP_MILLIS(creation_time) AS creation_time,
  TIMESTAMP_MILLIS(last_modified_time) AS last_modified
FROM `ventas_nettalco.__TABLES__`
ORDER BY row_count DESC;
```
![11](/grupo05_nettalco/PC3/evidencias_pc3/img_011.png)

## ‚úÖ B) Revisar esquema

``` sql
SELECT 
  table_name,
  column_name,
  data_type
FROM `ventas_nettalco.INFORMATION_SCHEMA.COLUMNS`
ORDER BY table_name;
```
![12](/grupo05_nettalco/PC3/evidencias_pc3/img_012.png)

## ‚úÖ C) Mostrar primeras filas

``` sql
SELECT *
FROM `ventas_nettalco.total_prendas_por_talla`
LIMIT 10;
```
![13](/grupo05_nettalco/PC3/evidencias_pc3/img_013.png)

------------------------------------------------------------------------

# üìä 4.5 Validaciones espec√≠ficas por tabla

### **1Ô∏è‚É£ Total de prendas**

``` sql
SELECT SUM(TOTAL_PRENDAS) AS total_prendas_suma
FROM `ventas_nettalco.total_prendas_por_talla`;
```
![14](/grupo05_nettalco/PC3/evidencias_pc3/img_014.png)

### **2Ô∏è‚É£ Top clientes por volumen**

``` sql
SELECT 
  TCODICLIE,
  TOTAL_PRENDAS
FROM `ventas_nettalco.volumen_ventas_por_cliente`
ORDER BY TOTAL_PRENDAS DESC
LIMIT 10;
```
![15](/grupo05_nettalco/PC3/evidencias_pc3/img_015.png)

### **3Ô∏è‚É£ Validaci√≥n por franja horaria**

``` sql
SELECT 
  FRANJA_HORARIA,
  COUNT(*) AS registros,
  SUM(TOTAL_PRENDAS) AS total
FROM `ventas_nettalco.tendencias_ventas_por_franja_horaria`
GROUP BY FRANJA_HORARIA;
```
![16](/grupo05_nettalco/PC3/evidencias_pc3/img_016.png)

### **4Ô∏è‚É£ Productos m√°s vendidos**

``` sql
SELECT 
  ESTILO,
  TOTAL_PRENDAS
FROM `ventas_nettalco.productos_mas_vendidos`
ORDER BY TOTAL_PRENDAS DESC
LIMIT 15;
```
![17](/grupo05_nettalco/PC3/evidencias_pc3/img_017.png)

### **5Ô∏è‚É£ Eficiencia operativa**

``` sql
SELECT 
  MIN(EFICIENCIA_PORCENTUAL) AS min_ef,
  MAX(EFICIENCIA_PORCENTUAL) AS max_ef
FROM `ventas_nettalco.eficiencia_operativa`;
```

### **6Ô∏è‚É£ Tendencias con promedio m√≥vil**

``` sql
SELECT 
  DATE(FECHA_TERMINO_TS) AS FECHA_TERMINO,
  ESTILO,
  TOTAL_PRENDAS,
  PROMEDIO_MOVIL
FROM `ventas_nettalco.prediccion_ventas`
ORDER BY FECHA_TERMINO DESC
LIMIT 20;
```
![18](/grupo05_nettalco/PC3/evidencias_pc3/img_018.png)

### **7Ô∏è‚É£ Comportamiento del cliente**

``` sql
SELECT
  TCODICLIE,
  FRECUENCIA_COMPRA,
  PROMEDIO_PRENDAS
FROM `ventas_nettalco.comportamiento_clientes`
ORDER BY FRECUENCIA_COMPRA DESC;
```
![19](/grupo05_nettalco/PC3/evidencias_pc3/img_019.png)

------------------------------------------------------------------------

# üß© 4.6 Conclusi√≥n

BigQuery permiti√≥ validar que:

‚úî Las tablas se cargaron exitosamente\
‚úî Los tipos fueron detectados correctamente\
‚úî Los c√°lculos de PySpark coinciden\
‚úî Los datos est√°n listos para visualizaci√≥n en Looker Studio

Esta fase asegura un flujo de Big Data estable y validado en GCP.

---

## üóÇÔ∏è 5. Dashboard en Looker
Los resultados del procesamiento se visualizaron en la siguiente herramienta:
![9](/grupo05_nettalco/PC3/evidencias_pc3/img_009.png)

**Link del Dashboard:** [Dashboard Looker Studio](https://lookerstudio.google.com/u/0/reporting/9139c4d1-2f52-4bd1-9e86-97b7554b2d58)

## üí∞ 6. Matriz de Costos y Optimizaci√≥n Financiera

Un componente crucial de cualquier despliegue en la nube es la gesti√≥n y optimizaci√≥n de costes. En esta secci√≥n, presentamos la matriz de costos actual del stack de Big Data, su desglose y las estrategias propuestas para lograr un ahorro significativo.

### a. Proyecci√≥n de Costos (Matriz Resumen)

La siguiente matriz resume la situaci√≥n financiera actual del proyecto y el potencial de ahorro estimado al implementar las pol√≠ticas de optimizaci√≥n.

*Evidencia:* Matriz de Costos y Proyecci√≥n de Uso (Cifras expresadas en Soles Peruanos: S/).

![20](/grupo05_nettalco/PC3/evidencias_pc3/img_020.png)

| Indicador | Valor Mensual | Valor Anual Estimado |
| :--- | :--- | :--- |
| *Costo Mensual Actual* | S/ 1,625.63 | - |
| *Costo Anual Estimado* | - | S/ 19,507.56 |
| *Ahorro Potencial* | S/ 956.07/mes | - |
| *Costo Optimizado (Estimado)* | S/ 669.56/mes | - |

### b. Desglose de Costos por Servicio

El an√°lisis inicial revela que el *Procesamiento* (Dataproc) es el componente que representa la mayor parte del gasto (85.4%), seguido por la *Transferencia* de datos.

*Evidencia:* Desglose detallado de Costos por Servicio (Almacenamiento, Procesamiento y Transferencia).

![21](/grupo05_nettalco/PC3/evidencias_pc3/img_021.png)

| Servicio | Almacenamiento (S/) | Procesamiento (S/) | Transferencia (S/) | Total Mensual (S/) |
| :--- | :--- | :--- | :--- | :--- |
| *BigQuery* | S/ 88.13 | S/ 468.75 | S/ 56.25 | S/ 613.13 |
| *Cloud Storage* | S/ 43.13 | S/ 0.00 | S/ 31.88 | S/ 75.00 |
| *Dataproc (Spark)* | S/ 0.00 | S/ 918.75 | S/ 18.75 | S/ 937.50 |
| *Looker Studio* | S/ 0.00 | S/ 0.00 | S/ 0.00 | S/ 0.00 |
| *TOTAL* | *S/ 131.26 (8.1%)* | *S/ 1,387.50 (85.4%)* | *S/ 106.88 (6.6%)* | *S/ 1,625.63* |

### c. Propuesta de Optimizaci√≥n de Costos

Dado que el procesamiento (Dataproc) es el principal impulsor de costos, las siguientes estrategias se enfocan en la eficiencia del c√≥mputo y el ciclo de vida de los datos, con un potencial de ahorro total de *S/ 956.07 mensuales*.

*Evidencia:* Propuesta de Optimizaci√≥n de Costos detallando las estrategias, el ahorro estimado y el porcentaje de descuento.

![22](/grupo05_nettalco/PC3/evidencias_pc3/img_022.png)

| Estrategia de Optimizaci√≥n | Implementaci√≥n | Ahorro Mensual Estimado | % Ahorro |
| :--- | :--- | :--- | :--- |
| *Instancias Preemptibles en Dataproc* | Configurar workers como VMs preemptibles para trabajos tolerantes a fallos. | S/ 551.25 | -60% |
| *Cl√∫ster con *Autoscaling** | Escalar autom√°ticamente seg√∫n demanda y programar el apagado en horarios inactivos. | S/ 229.69 | -25% |
| *Lifecycle Policies en Cloud Storage* | Mover datos antiguos a clases de almacenamiento fr√≠as (Nearline/Coldline) despu√©s de 90 d√≠as. | S/ 21.56 | -50% |
| *Particionamiento en BigQuery* | Particionar tablas por fecha para reducir la cantidad de datos escaneados en consultas. | S/ 140.63 | -30% |
| *Compresi√≥n de Archivos CSV* | Convertir los archivos CSV a un formato columnar comprimido (Parquet o Avro). | S/ 12.94 | -30% |

---
