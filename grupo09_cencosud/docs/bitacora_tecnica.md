# üìú BIT√ÅCORA DEL PROYECTO ETL MIGRACI√ìN A GCP

**Proyecto:** ETL de Ventas Retail (Hadoop/Hive a GCP/BigQuery)
**Fecha de Inicio:** 2025-11-15
[cite_start]**Equipo:** Grupo 9 (Cabana Cazani Gabriel, Larico Cruz Diego) [cite: 3, 4]
[cite_start]**Infraestructura Anterior:** Hortonworks HDP Sandbox (Hive, Spark, HDFS) [cite: 6]
**Infraestructura Nueva:** Google Cloud Platform (BigQuery, Dataproc Serverless, GCS)

---

## 1. FASE DE PREPARACI√ìN E INGESTA (RAW)

Esta fase se centr√≥ en establecer el entorno Serverless en GCP y replicar la ingesta de datos CSV sucios.

### Decisiones Clave

* **Data Warehouse:** Se eligi√≥ **BigQuery (BQ)** como sustituto de Hive/HDFS para el cat√°logo y el almacenamiento final.
* **Dataset:** Se cre√≥ el dataset **`dataset_si807_g9`** para alojar todas las capas (RAW, CURATED, ANALYTICS).
* [cite_start]**Almacenamiento:** Se utiliz√≥ **Cloud Storage (GCS)** (`mi-etl-proyecto-2025`) para alojar los archivos CSV y Parquet, replicando la estructura l√≥gica `/raw/`, `/curated/`, `/analytics/` que se usaba en HDFS[cite: 10, 11, 67, 68].

### Tareas y Problemas Resueltos

| Tarea | Estado | Observaci√≥n / Soluci√≥n |
| :--- | :--- | :--- |
| **Creaci√≥n de Tablas RAW (DDL)** | ‚úîÔ∏è Completada | [cite_start]Se ejecut√≥ `CREATE EXTERNAL TABLE` en BigQuery[cite: 162], apuntando a los CSV en GCS. [cite_start]Se us√≥ `skip_leading_rows = 1` para replicar la propiedad `TBLPROPERTIES ("skip.header.line.count"="1")` de Hive[cite: 190]. |
| **Error de Ingesta (Lectura CSV)** | ‚úîÔ∏è Resuelto | **Problema:** Las tablas externas en BQ no mostraban datos (estaban vac√≠as) al hacer `SELECT`. Se diagnostic√≥ que BQ fallaba al intentar interpretar los tipos de datos (`INT`, `FLOAT`) del CSV sucio. <br> **Soluci√≥n:** Se modific√≥ el DDL de la Capa RAW para definir **todos los campos como `STRING`**. Esto forz√≥ a BQ a leer los datos sin fallar y posterg√≥ el *casting* (conversi√≥n de tipos) a la capa Spark (Dataproc). |
| **Correcci√≥n de Tipos BQ** | ‚úîÔ∏è Resuelto | Se confirm√≥ que el tipo `DOUBLE` de Hive no existe en BigQuery. Se ajustaron los DDLs de las capas CURATED y ANALYTICS para usar **`FLOAT64`** para todos los campos decimales (ej: `monto_venta_neta`). |

---

## 2. FASE DE TRANSFORMACI√ìN (CURATED)

Esta fase se centr√≥ en migrar la l√≥gica de limpieza PySpark a un entorno Serverless y cargar los resultados a la capa CURATED de BigQuery.

### Tareas y Problemas Resueltos

| Tarea | Estado | Observaci√≥n / Soluci√≥n |
| :--- | :--- | :--- |
| **Adaptaci√≥n de C√≥digo Spark** | ‚úîÔ∏è Completada | Se consolid√≥ la l√≥gica de limpieza y transformaci√≥n de las 6 tablas en un script aut√≥nomo (`etl_curated_job.py`). Se adaptaron las sentencias de Hive (`spark.table(default.tabla)`) a BigQuery (`spark.table(dataset_si807_g9.tabla)`). |
| **Creaci√≥n de Tablas CURATED/ANALYTICS** | ‚úîÔ∏è Completada | [cite_start]Se ejecut√≥ el DDL (`CREATE TABLE`) para crear las estructuras vac√≠as en `dataset_si807_g9`, utilizando tipos de datos limpios (`DATE`, `INT64`, `FLOAT64`)[cite: 315]. |
| **Error de Conectividad de Red (Dataproc)** | ‚úîÔ∏è Resuelto | **Problema:** El env√≠o del Lote PySpark fall√≥ con el error "No hay ninguna red local disponible". <br> **Soluci√≥n:** Se verific√≥ la red VPC y se configur√≥ correctamente la subred, confirmando que la **Compute Engine Service Account** tuviera el rol **Usuario de red de Compute** (`roles/compute.networkUser`). |
| **Error de Sintaxis CLI (gcloud)** | ‚úîÔ∏è Resuelto | **Problema:** El comando `gcloud dataproc` no reconoc√≠a los flags `--executor-cores` o `--executor-memory`. <br> **Soluci√≥n:** Se corrigi√≥ la sintaxis, pasando los recursos del ejecutor a trav√©s del flag **`--properties`** (Ej: `--properties=spark.executor.cores=4,spark.executor.memory=16g`). |
| **Ejecuci√≥n y Carga** | ‚úîÔ∏è Completada | [cite_start]Se ejecut√≥ el Job de Lote (Batch) Serverless, que transform√≥ la data RAW y la carg√≥ a la Capa CURATED en BigQuery, usando el m√©todo `df.write.format("bigquery")...`[cite: 358]. |

---

## 3. FASE DE ANAL√çTICA Y VISUALIZACI√ìN

Esta fase se enfoca en el uso del Data Warehouse para el consumo de BI.

### Tareas y Pr√≥ximos Pasos

| Tarea | Estado | Observaci√≥n / Soluci√≥n |
| :--- | :--- | :--- |
| **Creaci√≥n del Cubo OLAP (ANALYTICS)** | ‚úîÔ∏è Completada | [cite_start]Se ejecut√≥ la consulta SQL de agregaci√≥n (`INSERT INTO`) en BigQuery para llenar la tabla `resumen_ventas_analytics` [cite: 457-485]. |
| **Conexi√≥n a Looker Studio** | ‚úîÔ∏è Completada | Se conect√≥ la tabla `resumen_ventas_analytics` como fuente principal. [cite_start]Se elimin√≥ la necesidad de ODBC que se usaba para Power BI[cite: 538]. |
| **Desarrollo de KPIs** | üèóÔ∏è En Progreso | El desarrollo de KPIs complejos (ej: "Ticket Promedio por Canal") requiere la **Combinaci√≥n de Datos (Data Blending)** en Looker Studio, uniendo la tabla agregada (`resumen_ventas_analytics`) con tablas de dimensiones (`dim_tienda_canal_curated`). |
